# ml-service/server.py
import grpc
from concurrent import futures
import categorizer_pb2
import categorizer_pb2_grpc
import fasttext
import psycopg2
import psycopg2.extras
import os
import re
import logging
from datetime import datetime
from threading import Lock
import time
import hashlib
import json
import threading  # ‚Üê –î–û–ë–ê–í–ò–¢–¨ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ _start_watcher)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DB_URL = os.getenv('DATABASE_URL', 'postgres://user:pass@db:5432/mydb')
MODEL_DIR = '/app/models'
MODEL_PATH = os.path.join(MODEL_DIR, 'model.bin')
DATA_PATH = os.path.join(MODEL_DIR, 'train.txt')
META_PATH = os.path.join(MODEL_DIR, 'metadata.json')

os.makedirs(MODEL_DIR, exist_ok=True)


class DatabaseReader:
    def __init__(self, db_url):
        self.db_url = db_url
        self.last_trained_at = self._load_last_trained()
    
    def _load_last_trained(self) -> datetime:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        meta_path = os.path.join(MODEL_DIR, 'training_meta.json')
        if os.path.exists(meta_path):
            with open(meta_path, 'r') as f:
                data = json.load(f)
                return datetime.fromisoformat(data.get('last_trained_at', '1970-01-01T00:00:00'))
        return datetime(1970, 1, 1)
    
    def save_last_trained(self, dt: datetime):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"""
        meta_path = os.path.join(MODEL_DIR, 'training_meta.json')
        data = {
            'last_trained_at': dt.isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        with open(meta_path, 'w') as f:
            json.dump(data, f, indent=2)
    
    def _get_conn(self):
        """‚Üê –î–û–ë–ê–í–ò–¢–¨: —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î"""
        return psycopg2.connect(self.db_url)
    
    def get_all_categories(self):
        """‚Üê –î–û–ë–ê–í–ò–¢–¨: –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ _full_train)"""
        conn = self._get_conn()
        try:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT id, name, icon, color 
                    FROM categories 
                    ORDER BY name
                """)
                categories = []
                for row in cur.fetchall():
                    cat = dict(row)
                    cur.execute("""
                        SELECT text FROM examples 
                        WHERE category_id = %s
                    """, (cat['id'],))
                    cat['examples'] = [r['text'] for r in cur.fetchall()]
                    categories.append(cat)
                return categories
        finally:
            conn.close()
    
    def get_new_examples(self, since: datetime) -> list[dict]:  # ‚Üê –ò–°–ü–†–ê–í–ò–¢–¨: list[dict] –≤–º–µ—Å—Ç–æ List[Dict]
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Å –º–æ–º–µ–Ω—Ç–∞ last_trained_at"""
        conn = self._get_conn()
        try:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT e.category_id, e.text, c.name, c.icon, c.color
                    FROM examples e
                    JOIN categories c ON e.category_id = c.id
                    WHERE e.created_at > %s
                    ORDER BY e.created_at
                """, (since,))
                return [dict(row) for row in cur.fetchall()]
        finally:
            conn.close()
    
    def get_examples_count_since(self, since: datetime) -> int:
        """–°–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        conn = self._get_conn()
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM examples WHERE created_at > %s", (since,))
                return cur.fetchone()[0]
        finally:
            conn.close()


class FastTextCategorizerServicer:
    def __init__(self):
        self.db = DatabaseReader(DB_URL)
        self.model = None
        self.is_training = False
        self.training_lock = Lock()
        self.categories_cache = []
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.lr = 0.5  # ‚Üê –î–û–ë–ê–í–ò–¢–¨: learning rate (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –æ–±—É—á–µ–Ω–∏–∏)
        self.word_ngrams = 2
        self.dim = 100
        self.epoch = 100
        self.incremental_epoch = 5
        
        self._init_model()
        self._start_watcher()
    
    def _clean_text(self, text):  # ‚Üê –î–û–ë–ê–í–ò–¢–¨: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ _incremental_train –∏ Predict
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        text = text.lower().strip()
        text = re.sub(r'\d+[\s]*[‚ÇΩ—Ä—É–±$‚Ç¨]?', '', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        return ' '.join(text.split())
    
    def _generate_training_file(self, categories):  # ‚Üê –î–û–ë–ê–í–ò–¢–¨: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ _full_train
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –æ–±—É—á–µ–Ω–∏—è"""
        lines = []
        for cat in categories:
            for example in cat.get('examples', []):
                clean = self._clean_text(example)
                if clean:
                    lines.append(f"__label__{cat['id']} {clean}")
        
        with open(DATA_PATH, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        return len(lines)
    
    def _load_model(self):  # ‚Üê –î–û–ë–ê–í–ò–¢–¨: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ _init_model
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            self.model = fasttext.load_model(MODEL_PATH)
            self.categories_cache = self.db.get_all_categories()
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(self.categories_cache)}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            raise
    
    def _init_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω–µ—Ç –º–æ–¥–µ–ª–∏, –∏–Ω–∞—á–µ –∑–∞–≥—Ä—É–∑–∫–∞"""
        if os.path.exists(MODEL_PATH):
            self._load_model()
            new_count = self.db.get_examples_count_since(self.db.last_trained_at)
            if new_count > 0:
                logger.info(f"üì¨ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {new_count} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å {self.db.last_trained_at}")
                self._incremental_train()
        else:
            logger.info("üÜï –ü–µ—Ä–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è, –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
            self._full_train()
    
    def _full_train(self):
        """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with self.training_lock:
            self.is_training = True
            try:
                categories = self.db.get_all_categories()
                if not categories:
                    logger.warning("‚ö†Ô∏è –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ë–î!")
                    return False
                
                count = self._generate_training_file(categories)
                logger.info(f"üìö –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {count} –ø—Ä–∏–º–µ—Ä–æ–≤")
                
                self.model = fasttext.train_supervised(
                    input=DATA_PATH,
                    lr=self.lr,
                    epoch=self.epoch,
                    wordNgrams=self.word_ngrams,
                    dim=self.dim,
                    loss='softmax'
                )
                
                self._save_model_and_meta(categories, count, "full")
                return True
            finally:
                self.is_training = False
    
    def _incremental_train(self):
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with self.training_lock:
            self.is_training = True
            try:
                new_examples = self.db.get_new_examples(self.db.last_trained_at)
                if not new_examples:
                    logger.info("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                    return False
                
                lines = []
                for ex in new_examples:
                    clean = self._clean_text(ex['text'])
                    if clean:
                        lines.append(f"__label__{ex['category_id']} {clean}")
                
                temp_path = os.path.join(MODEL_DIR, 'incremental_train.txt')
                with open(temp_path, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(lines))
                
                logger.info(f"üìà –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {len(lines)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ + –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                combined_path = os.path.join(MODEL_DIR, 'combined_train.txt')
                with open(DATA_PATH, 'r', encoding='utf-8') as f_old, \
                     open(temp_path, 'r', encoding='utf-8') as f_new, \
                     open(combined_path, 'w', encoding='utf-8') as f_out:
                    old_data = f_old.read()
                    new_data = f_new.read()
                    f_out.write(old_data + ('\n' if old_data and new_data else '') + new_data)
                
                self.model = fasttext.train_supervised(
                    input=combined_path,
                    lr=self.lr,
                    epoch=self.incremental_epoch,
                    wordNgrams=self.word_ngrams,
                    dim=self.dim,
                    loss='softmax'
                )
                
                os.replace(combined_path, DATA_PATH)
                
                self._save_model_and_meta(
                    self.db.get_all_categories(), 
                    len(lines), 
                    "incremental"
                )
                
                return True
                
            finally:
                self.is_training = False
    
    def _save_model_and_meta(self, categories, count, train_type):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        self.model.save_model(MODEL_PATH)
        self.categories_cache = categories
        
        now = datetime.now()
        self.db.save_last_trained(now)
        
        with open(META_PATH, 'w') as f:
            json.dump({
                'trained_at': now.isoformat(),
                'train_type': train_type,
                'examples_count': count,
                'categories_count': len(categories),
                'params': {
                    'lr': self.lr,
                    'epoch': self.epoch if train_type == 'full' else self.incremental_epoch,
                    'wordNgrams': self.word_ngrams,
                    'dim': self.dim
                }
            }, f, indent=2)
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({train_type})")
    
    def _start_watcher(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑ –≤ 30 —Å–µ–∫—É–Ω–¥"""
        def watch():
            while True:
                time.sleep(30)
                try:
                    if self.is_training:
                        continue
                    
                    new_count = self.db.get_examples_count_since(self.db.last_trained_at)
                    if new_count > 5:
                        logger.info(f"üîÑ Watcher: {new_count} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
                        self._incremental_train()
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ watcher: {e}")
        
        threading.Thread(target=watch, daemon=True).start()
        logger.info("üëÅÔ∏è Watcher –∑–∞–ø—É—â–µ–Ω (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 30—Å)")
    
    # ============ gRPC –º–µ—Ç–æ–¥—ã ============
    
    def Predict(self, request, context):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if self.is_training:
            context.set_code(grpc.StatusCode.UNAVAILABLE)
            context.set_details("–ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è, –ø–æ–¥–æ–∂–¥–∏—Ç–µ 5 —Å–µ–∫—É–Ω–¥")
            return categorizer_pb2.PredictResponse()
        
        if not self.model:
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return categorizer_pb2.PredictResponse()
        
        clean = self._clean_text(request.text)
        if not clean:
            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
            context.set_details("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
            return categorizer_pb2.PredictResponse()
        
        try:
            labels, probs = self.model.predict(clean, k=3)
            
            alternatives = []
            for label, prob in zip(labels, probs):
                cat_id = label.replace('__label__', '')
                
                cat_meta = next(
                    (c for c in self.categories_cache if c['id'] == cat_id),
                    {'name': cat_id, 'icon': '‚ùì', 'color': '#CCCCCC'}
                )
                
                alternatives.append(categorizer_pb2.PredictionResult(
                    category_id=cat_id,
                    category_name=cat_meta['name'],
                    category_icon=cat_meta['icon'],
                    category_color=cat_meta['color'],
                    confidence=float(prob)
                ))
            
            primary = alternatives[0] if alternatives else None
            
            return categorizer_pb2.PredictResponse(
                primary=primary,
                alternatives=alternatives[1:],
                needs_confirmation=(primary.confidence < 0.7) if primary else True,
                source='fasttext'
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return categorizer_pb2.PredictResponse()
    
    def ForceRetrain(self, request, context):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        force_full = request.full
        
        if force_full:
            success = self._full_train()
            msg = "–ü–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ"
        else:
            success = self._incremental_train()
            msg = "–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ" if success else "–ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
        
        return categorizer_pb2.StatusResponse(
            success=success,
            message=msg,
            categories_count=len(self.categories_cache),
            is_training=self.is_training
        )
    
    def GetStatus(self, request, context):
        """–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return categorizer_pb2.StatusResponse(
            success=True,
            message="–°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç",
            categories_count=len(self.categories_cache),
            is_training=self.is_training
        )
    
    def GetModelInfo(self, request, context):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
        info = {}
        if os.path.exists(META_PATH):
            with open(META_PATH, 'r') as f:
                info = json.load(f)
        
        return categorizer_pb2.ModelInfoResponse(
            model_path=MODEL_PATH,
            data_hash='',  # ‚Üê –£–ë–†–ê–¢–¨: self.last_data_hash –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            categories_count=len(self.categories_cache),
            is_training=self.is_training,
            metadata=json.dumps(info)
        )


def serve():
    """–ó–∞–ø—É—Å–∫ gRPC —Å–µ—Ä–≤–µ—Ä–∞"""
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    
    servicer = FastTextCategorizerServicer()
    categorizer_pb2_grpc.add_ExpenseCategorizerServicer_to_server(servicer, server)
    
    server.add_insecure_port('[::]:50051')
    server.start()
    
    logger.info("üöÄ gRPC —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É 50051")
    logger.info(f"üìä PostgreSQL: {DB_URL.replace('pass', '***')}")
    
    server.wait_for_termination()


if __name__ == '__main__':
    time.sleep(3)
    serve()